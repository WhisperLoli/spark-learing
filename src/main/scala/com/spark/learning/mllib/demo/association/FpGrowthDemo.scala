package com.spark.learning.mllib.demo.association

import com.spark.learning.mllib.demo.spark
import org.apache.log4j.{Level, Logger}
import org.apache.spark.internal.Logging
import org.apache.spark.ml.fpm.FPGrowth

/**
  * ä¸€èˆ¬æ¥è¯´ï¼Œæ”¯æŒåº¦é«˜çš„æ•°æ®ä¸ä¸€å®šæ„æˆé¢‘ç¹é¡¹é›†ï¼Œä½†æ˜¯æ”¯æŒåº¦å¤ªä½çš„æ•°æ®è‚¯å®šä¸æ„æˆé¢‘ç¹é¡¹é›†
  * Support(AB) = P(AB) = nums(AB)/nums(all)
  *
  * ç½®ä¿¡åº¦ä½“ç°äº†ä¸€ä¸ªæ•°æ®å‡ºç°åï¼Œå¦ä¸€ä¸ªæ•°æ®å‡ºç°çš„æ¦‚ç‡ï¼Œæˆ–è€…è¯´æ•°æ®çš„æ¡ä»¶æ¦‚ç‡, ğ‘ƒ(A|B)=ğ‘ƒ(AB)/ğ‘ƒ(B)
  *
  * åœ¨è´­ç‰©æ•°æ®ä¸­ï¼Œçº¸å·¾å¯¹åº”é¸¡çˆªçš„ç½®ä¿¡åº¦ä¸º90%ï¼Œæ”¯æŒåº¦ä¸º1%ã€‚
  * åˆ™æ„å‘³ç€åœ¨è´­ç‰©æ•°æ®ä¸­ï¼Œæ€»å…±æœ‰1%çš„ç”¨æˆ·æ—¢ä¹°é¸¡çˆªåˆä¹°çº¸å·¾;åŒæ—¶ä¹°é¸¡çˆªçš„ç”¨æˆ·ä¸­æœ‰90%çš„ç”¨æˆ·è´­ä¹°çº¸å·¾ã€‚
  *
  * é¢‘ç¹é¡¹é›†æŒ–æ˜ï¼š
  *   Aprioriç®—æ³•: æ¯æ¬¡æ‰«ææ•°æ®é›†å¯»æ‰¾é¡¹æ”¯æŒåº¦ï¼Œè¿‡æ»¤æ‰ä½äºMinSupportçš„é¡¹ï¼Œé¡¹å€¼ä¾æ¬¡å¢å¤§ï¼Œké¡¹é›†ä¾èµ–k-1é¡¹é›†ç”Ÿæˆ
  *   æ¯å¢å¤§ä¸€æ¬¡ï¼Œéƒ½éœ€è¦è¿‡æ»¤ä½äºMinSupportçš„é¡¹ï¼Œæ¯æ¬¡éƒ½éœ€è¦æ‰«æä¸€æ¬¡æ•°æ®é›†ï¼Œæ—¶é—´å¤æ‚åº¦O(kn)ï¼Œkä¸ºé¢‘ç¹é¡¹æ•°
  *
  *   FP-Growthç®—æ³•: åœ¨æ„å»º FP æ ‘æ—¶åªéœ€è¦æ‰«ææ•°æ®é›†ä¸¤æ¬¡ã€‚è¿™ç§ç‰¹æ€§ä½¿å¾— FP-Growth ç®—æ³•æ¯” Aprior ç®—æ³•é€Ÿåº¦å¿«ã€‚
  *   FP æ ‘æ˜¯ä¸€ç§å‰ç¼€æ ‘, æ—¶é—´å¤æ‚åº¦O(n)ï¼Œå¸¸æ•°2å¯ä»¥å¿½ç•¥
  *   ç»Ÿè®¡æ•°æ®é›†ä¸­å„ä¸ªå…ƒç´ å‡ºç°çš„é¢‘æ•°ï¼Œå°†é¢‘æ•°å°äºMinSupportçš„å…ƒç´ ä»æ•°æ®é›†ä¸­åˆ é™¤ï¼Œå°†æ–°æ•°æ®é›†ä¸­çš„å„æ¡è®°å½•æŒ‰é¡¹å‡ºç°é¢‘æ•°é™åºæ’åˆ—
  *   éå†æ–°æ•°æ®é›†ï¼Œæ„å»ºFPæ ‘ï¼Œå†æ ¹æ®FPæ ‘æ„é€ é¢‘ç¹é¡¹é›†
  *
  * å…³è”è§„åˆ™ï¼š
  *   åœ¨é¢‘ç¹é¡¹é›†çš„åŸºç¡€ä¸Šå‘ç°å…³è”è§„åˆ™ï¼Œå…³è”è§„åˆ™æŒ–æ˜é¦–å…ˆéœ€è¦å¯¹ä¸Šæ–‡å¾—åˆ°çš„é¢‘ç¹é¡¹é›†æ„å»ºæ‰€æœ‰å¯èƒ½çš„è§„åˆ™ï¼Œ
  *   ç„¶åå¯¹æ¯æ¡è§„åˆ™é€ä¸ªè®¡ç®—ç½®ä¿¡åº¦ï¼Œè¾“å‡ºç½®ä¿¡åº¦å¤§äºæœ€å°ç½®ä¿¡åº¦çš„æ‰€æœ‰è§„åˆ™
  *   ä¸‹è¿°è§„åˆ™çš„å¯ä¿¡åº¦å³ä¸ºç½®ä¿¡åº¦
  *   AB -> C
  *
  */
object FpGrowthDemo extends App with Logging {
  import spark.implicits._

  // åªè®¾ç½®sparkçš„logçº§åˆ«ä¸ºWARNï¼Œå…¶ä»–logçº§åˆ«ä¸ºinfo
  Logger.getLogger("org.apache.spark").setLevel(Level.WARN)

  val dataset = spark.createDataset(Seq(
    "1 2 5",
    "1 2 3 5",
    "1 2")
  ).map(t => t.split(" ")).toDF("items").cache()

  val fpgrowth = new FPGrowth()
    .setItemsCol("items")
    // æœ€å°æ”¯æŒåº¦ï¼Œitemæ•°ç›®ï¼Œæ”¯æŒåº¦å°±æ˜¯å‡ ä¸ªå…³è”çš„æ•°æ®åœ¨æ•°æ®é›†ä¸­å‡ºç°çš„æ¬¡æ•°å æ€»æ•°æ®é›†çš„æ¯”é‡
    .setMinSupport(0.6)
    // æœ€å°ç½®ä¿¡åº¦ï¼Œå½“å‡ºç°æŸå‡ é¡¹itemï¼Œå‡ºç°å¦ä¸€é¡¹itemçš„ç½®ä¿¡åº¦è¦é«˜äºè®¾ç½®çš„å€¼ï¼Œæ‰ä¼šç®—å…¥é¢‘ç¹é¡¹ä¸­
    .setMinConfidence(0.6)

  val model = fpgrowth.fit(dataset)

  // é¢‘ç¹é¡¹é›†
  model.freqItemsets.show()

  // åº”ç”¨å…³è”è§„åˆ™
  model.associationRules.show()

  // transform examines the input items against all the association rules and summarize the
  // consequents as prediction
  model.transform(dataset).show()

  log.info("finish")
}
